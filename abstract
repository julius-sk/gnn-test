Abstract
Sparse General Matrix-Matrix Multiplication (SpGEMM) is a fundamental operation in numerous scientific computing and data analytics applications, often bottlenecked by irregular memory access patterns. This paper introduces the Acceleration of Indirect memory Access (AIA) technique, a novel approach to optimize SpGEMM on HBM GPUs. We develop a Ranged Indirect Access method and integrate it with a hash-based SpGEMM algorithm. Our Processing-Near-HBM based SpGEMM implementation demonstrates significant performance improvements over state-of-the-art methods, particularly in handling complex, application-specific workloads. We evaluate our approach on various graph workloads, including Graph Neural Networks (GNNs), graph contraction, and Markov clustering, showcasing its practical applicability. For GNN applications, our experimental results demonstrate that optimized SpGEMM operations can achieve substantial acceleration, with recent advances showing 3.22×-4.24× speedup over state-of-the-art frameworks while maintaining comparable accuracy. Results show consistent enhancements in cache utilization (up to 88% L1 cache hit ratio improvement), runtime performance (2.5× GFLOPS improvement on Wind Tunnel dataset), and computational throughput across diverse datasets. Graph application benchmarks demonstrate 2.5%-15.8% performance improvements, with Markov clustering achieving up to 15.8% speedup on complex datasets. Our findings position AIA as a powerful tool for high-performance graph processing on GPU architectures, with particular effectiveness for iterative algorithms that rely heavily on sparse matrix operations.
Index Terms—Indirect memory Access, Sparse General Matrix-Matrix Multiplication, GPU, Near Memory Processing, Graph Neural Networks
