Sparse General Matrix-Matrix Multiplication (SpGEMM) is a fundamental operation in numerous scientific computing and data analytics applications, often bottlenecked by irregular memory access patterns. This paper introduces the Acceleration of Indirect memory Access (AIA) technique, a novel approach to optimize SpGEMM on HBM GPUs. We develop a Ranged Indirect Access method and integrate it with a hash-based SpGEMM algorithm. Our Processing-Near-HBM based SpGEMM implementation demonstrates significant performance improvements over state-of-the-art methods, particularly in handling complex, application-specific workloads. We evaluate our approach on various graph workloads, including Graph Neural Networks (GNNs), graph contraction, and Markov clustering, showcasing its practical applicability. Our hash-based SpGEMM with AIA achieves an average of 77.27% speedup over cuSPARSE and 12.53% improvement over hash-based SpGEMM without AIA across diverse matrix datasets. For GNN training applications, our approach delivers 53.2% faster training time compared to cuSPARSE baselines, with a hybrid implementation (AIA forward + optimized backward) achieving up to 67.3% speedup while maintaining comparable accuracy. Results show consistent enhancements in cache utilization (L1 cache hit ratio improvements from 64.41% to 88.15%), runtime performance, and computational throughput across diverse datasets. Graph application benchmarks demonstrate 2.5%-15.8% performance improvements, with Markov clustering achieving up to 15.8% speedup and our AIA gathering phase constituting only 17.2% of total execution time, indicating significant optimization potential. Our findings position AIA as a powerful tool for high-performance graph processing on GPU architectures, with particular effectiveness for iterative algorithms that rely heavily on sparse matrix operations.
