# I. INTRODUCTION

Sparse General Matrix-Matrix Multiplication (SpGEMM) plays a pivotal role in computational science, machine learning, and graph analysis. While general matrix-matrix multiplication (GEMM) multiplies a matrix A (m × k) with matrix B (k × n) to produce matrix C (m × n), many applications such as algebraic multigrid methods [1], multi-source breadth-first search [2], recursive formulations of all-pairs shortest-paths algorithms [3], clustering coefficient [4], graph contraction [5], Markov clustering [6], and **Graph Neural Networks (GNNs) [7]** require exploiting sparsity in both input and output matrices to optimize storage and computational efficiency. **Graph Neural Networks represent a particularly compelling application domain for SpGEMM optimization, as they fundamentally rely on sparse matrix operations for neighborhood aggregation and feature propagation across graph structures, with recent advances demonstrating that optimized sparse operations can achieve 3.22×-4.24× speedup over state-of-the-art frameworks [8].**

However, the efficient execution of SpGEMM on modern GPUs faces several fundamental challenges, despite GPUs offering superior peak floating-point performance and memory bandwidth compared to CPUs. The primary challenges arise from the irregular nature of sparse computations. While compressed representations of sparse matrices reduce memory footprint, they incur significant overheads in accessing and processing metadata. These challenges manifest in three critical aspects: (1) the unknown number of non-zero entries in the result matrix before computation, (2) expensive parallel insert operations at random positions in the result matrix, and (3) load balancing issues when handling input matrices with diverse sparsity structures. The irregular memory access patterns create a strong mismatch between data access patterns and memory layout, leading to limited spatial locality and cache effectiveness.

Previous GPU SpGEMM methods have attempted to address these challenges through various approaches, including dedicated hardware accelerators and software optimizations [9], [10]. However, these solutions have shown limited success, either performing well only for relatively regular sparse matrices or introducing significant memory overhead for specific sparsity patterns [1], [11]. Moreover, existing GPU implementations have struggled to consistently outperform well-optimized CPU approaches, highlighting the difficulty in fully utilizing GPU's massive parallelism for sparse matrix operations [12].

The advancement in 3D integration technologies has made the concept of coupling compute units close to the memory—called near memory processing (NMP) [13]—more viable. High Bandwidth Memory (HBM) [14] adopts a vertically stacked architecture that allows for greater capacity and bandwidth. Processing right at the "home" of data can significantly diminish the data movement problem of SpGEMM. Thus, we adopt the idea of processing near GPU HBM to tackle the challenges of SpGEMM.

In this paper, we propose and implement the Acceleration of Indirect Memory Access (AIA) system, a novel Processing-Near-HBM approach to enhance performance in handling indirect memory access patterns in SpGEMM operations. We introduce the AIA technique, designed to assist the primary computational core in sparse data computations, specifically targeting SpGEMM operations on HBM GPUs. Our main contributions include: **1) Development of a novel ranged indirect access technique for SpGEMM that effectively addresses the fundamental memory wall challenge in sparse matrix computations, transforming irregular memory access patterns into more efficient sequential streams and optimizing the two critical levels of indirection in SpGEMM;** **2) Integration of AIA functionality within GPU HBM architecture, leveraging high-bandwidth capabilities while mitigating traditional access pattern limitations, with AIA units placed between GPU cores and HBM stacks to enable efficient data gathering and significantly improve cache utilization—demonstrated by L1 cache hit ratio improvements from 64.41% to 75.14% in numeric phases and 64.66% to 88.15% in symbolic phases;** **3) Optimization of a sophisticated hash-based SpGEMM algorithm that effectively utilizes GPU resources through adaptive thread assignment and efficient shared memory hash tables, achieving up to 2.5× performance improvement over state-of-the-art cuSPARSE;** **4) Comprehensive evaluation demonstrating AIA's practical impact across diverse applications, with performance improvements up to 13.3% for Graph Contraction and 15.8% for Markov Clustering, and particularly notable results for GNN applications where MaxK-GNN systems achieve 3.22×-4.24× speedup over DGL frameworks and 4.15×-6.39× speedup over cuSPARSE implementations while maintaining comparable accuracy to state-of-the-art GNNs.** Our implementation achieves over 60 GFLOPS compared to 52 GFLOPS for non-AIA and less than 10 GFLOPS for cuSPARSE, validating AIA's scalability and efficiency for iterative graph algorithms requiring multiple SpGEMM operations.
